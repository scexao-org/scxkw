#!/bin/env python
from __future__ import annotations

import argparse
import typing as typ

import numpy as np
from pathlib import Path

import docopt
import logging
import tqdm
from tqdm.contrib.logging import logging_redirect_tqdm

from swmain.infra.logger import init_logger_autoname


from scxkw.config import GEN2PATH_NODELETE
from scxkw.tools import file_tools
from scxkw.tools.framelist_file_obj import FrameListFitsFileObj
from scxkw.tools.fits_file_obj import FitsFileObj

init_logger_autoname()
logg = logging.getLogger(__name__)

# prepare arg parsing
parser = argparse.ArgumentParser(
    "scxkw-fitsframe-consolidate",
    description="Consolidate fitsframes files back to fits files.",
    epilog="If you run into problems, contacnt vdeo or mlucas (TODO make undo script)."
)
parser.add_argument("folder", type=Path, help="Root folder containing the `agen2` subfolder")
parser.add_argument("-p", "--num-procs", default=1, type=int, help="Multi-processing pool size (default is %(default)s)")


def write_func(fobjs_framelist, fobjs_fits_dict):
    fitsobj = file_tools.consolidate_framelist_to_fits(fobjs_framelist, fobjs_fits_dict)
    fitsobj.write_to_disk(try_flush_ram=True)
    # If the write is successful, we should get rid of the fitsframes, otherwise
    # We might consolidate it a second time...

    ### WARNING! the txt is shared!
    fobjs_framelist.disown_txt_file()
    fobjs_framelist.delete_from_disk()

def main():
    args = parser.parse_args()
    logg.debug("Parsed arguments:")
    logg.debug(args)

    folderPath = args.folder.absolute()
    logg.debug(f"Absolute path: {folderPath}")

    assert (folderPath.is_absolute() and folderPath.is_dir())


    vgen2_glob = f"{folderPath}/vgen2/*.fitsframes"
    vgen2_list = file_tools.make_fileobjs_from_globs([vgen2_glob], [], type_to_use=FrameListFitsFileObj)

    agen2_glob = f"{folderPath}/agen2/*.fitsframes"
    agen2_list = file_tools.make_fileobjs_from_globs([agen2_glob], [], type_to_use=FrameListFitsFileObj)
    fobjs_framelists = vgen2_list + agen2_list
    fobjs_framelists.sort(key=lambda fobj: fobj.get_start_unixtime_secs())

    set_needed_fits: set[str] = set()
    dict_needed_fits2fitsframes: dict[str, set[str]] = {}
    dict_needed_fitsframes2fits: dict[str, set[str]] = {}

    for fobj_fitsframes in fobjs_framelists:
        fobj_fitsframes._ensure_data_loaded()
        needed_fits: set[str] = set(fobj_fitsframes.data[:,0])
        # Maintain set of all needed fits files to consolidate.
        set_needed_fits.update(needed_fits)
        # Maintain dictionnary of forward and reverse relationship between fits and fitsframes
        dict_needed_fitsframes2fits[str(fobj_fitsframes.full_filepath)] = needed_fits
        for fits_fname in needed_fits:
            if not fits_fname in dict_needed_fitsframes2fits:
                dict_needed_fits2fitsframes[fits_fname] = set()
            dict_needed_fits2fitsframes[fits_fname].add(str(fobj_fitsframes.full_filepath))

    # Save time if we don't need to load all the set (little amount of .fitsframes remaining)
    fobjs_fits = [FitsFileObj(fullname=fname) for fname in set_needed_fits]
    fobjs_fits_dict = {str(fo.full_filepath): fo for fo in fobjs_fits}

    # Find trivial files where the consolidation is actually just a move. OR we could hardlink?
    fobjs_framelists_toremove: list[FrameListFitsFileObj] = []
    for fobj_fitsframes in fobjs_framelists:
        fpath_fitsframes = str(fobj_fitsframes.full_filepath)
        set_fits_needed_for_fobj = dict_needed_fitsframes2fits[fpath_fitsframes]
        if len(set_fits_needed_for_fobj) == 1:
            fpath_fits = list(set_fits_needed_for_fobj)[0]
            if len(dict_needed_fits2fitsframes[fpath_fits]) == 1:
                this_fobj_fits = fobjs_fits_dict[fpath_fits]
                if (fobj_fitsframes.get_nframes() == this_fobj_fits.get_nframes() and
                    np.all(fobj_fitsframes.data[:,1] == np.arange(this_fobj_fits.get_nframes()))):
                    logg.warning(f'Trivial duplicate found! '
                              f'{fpath_fitsframes} and {fpath_fits}.')
                    # Move the file - logically they have the same name so the .txt should be named ok
                    this_fobj_fits.move_file_to_streamname(fobj_fitsframes.stream_from_foldername)
                    fobj_fitsframes.disown_txt_file()
                    fobj_fitsframes.delete_from_disk()

                    fobjs_framelists_toremove.append(fobj_fitsframes)

    fobjs_framelists = [f for f in fobjs_framelists if (not f in fobjs_framelists_toremove)]

    import multiprocessing # Using multiprocessing is a bit faster, and also keeps the RAM in check
    # But careful: files may appear in timely disorder

    pbar = tqdm.trange(len(fobjs_framelists), desc="Consolidating")
    for slice_idx in range(len(fobjs_framelists) // 10 + 1):
        with logging_redirect_tqdm():
            jobs = []
            for fobjs_framelist in fobjs_framelists[10*slice_idx:10*slice_idx+10]:
                job = multiprocessing.Process(target=write_func, args=(fobjs_framelist, fobjs_fits_dict))
                jobs.append(job)
                job.start()

            for job in jobs:
                job.join()
                pbar.update()
    pbar.close()

if __name__ == "__main__":
    main()
